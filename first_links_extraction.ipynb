{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from math import ceil\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_main(link):\n",
    "    '''\n",
    "    a function that request the webpage and store it in response object. then passing lxml parser to parse over the webpage.\n",
    "    \n",
    "    here lxml parser defines the speed to parse the webpage.\n",
    "    \n",
    "    if it throws any errors in using lxml parser. just install it by: \" pip intall lxml \"\n",
    "    '''\n",
    "    \n",
    "    response = requests.get(link)\n",
    "    return(BeautifulSoup(response.text, 'lxml' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of internshala links\n",
    "\n",
    "links = ['https://internshala.com/internships/work-from-home-jobs', 'https://internshala.com/internships/internship-in-bangalore',\\\n",
    "        'https://internshala.com/internships/internship-in-hyderabad', 'https://internshala.com/internships/internship-in-odisha']\n",
    "\n",
    "# generalied locations based on links and its position\n",
    "\n",
    "loc_links = ['work_from_home', 'bangalore', 'hyderabad', 'odisha']\n",
    "\n",
    "row_heading = ['source', 'location', 'job_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp as file name by using time library and with prefix as internshala\n",
    "file_name = 'internshala_first_data_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + \".csv\" \n",
    "\n",
    "\n",
    "# opening file in write mode and connecting csv writer to file\n",
    "file = open(file_name,'w')\n",
    "writer = csv.writer(file)\n",
    "\n",
    "\n",
    "# initially writing header of csv file\n",
    "writer.writerow(row_heading)\n",
    "\n",
    "\n",
    "# looping over array of links with index value\n",
    "for index,url in enumerate(links):\n",
    "    \n",
    "    #passing main page to scape\n",
    "    soup = scrape_main(url)\n",
    "\n",
    "    #  to find number of job pages to scrape we need to get count of jobs available which is at heading in webpage.\n",
    "    #print(soup.find('div',{'class':'heading heading_4_6'}))\n",
    "    \n",
    "    # based on count of jobs - finding the number of pages available at one link in array of links.\n",
    "    pages = ceil(int(soup.find('div',{'class':'heading heading_4_6'}).text.split()[0])/40)\n",
    "    for page in range(pages):\n",
    "        \n",
    "        # now we need to scrape over pages under main url\n",
    "        base_url = url + \"/page-\"+str(page)\n",
    "        soup1= scrape_main(base_url)\n",
    "        \n",
    "        # firstly finding each single job in each page to find job link\n",
    "        for single_job in soup.find_all(\"div\", { \"class\": \"individual_internship\"}):\n",
    "            \n",
    "            if(single_job.find('div',{'class':'heading_4_5 profile'}) == None):\n",
    "                continue\n",
    "                \n",
    "            job_link = \"https://internshala.com\"\n",
    "            job_link += single_job.find('div',{'class':'heading_4_5 profile'}).a.get('href')\n",
    "        \n",
    "            source = 'internshala'\n",
    "            \n",
    "            location = loc_links[index]\n",
    "            \n",
    "            # writing all details to csv\n",
    "            writer.writerow([source, location, job_link])\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing csv file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>location</th>\n",
       "      <th>job_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>internshala</td>\n",
       "      <td>hyderabad</td>\n",
       "      <td>https://internshala.com/internship/detail/digi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>internshala</td>\n",
       "      <td>odisha</td>\n",
       "      <td>https://internshala.com/internship/detail/busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>internshala</td>\n",
       "      <td>odisha</td>\n",
       "      <td>https://internshala.com/internship/detail/cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>internshala</td>\n",
       "      <td>odisha</td>\n",
       "      <td>https://internshala.com/internship/detail/web-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>internshala</td>\n",
       "      <td>odisha</td>\n",
       "      <td>https://internshala.com/internship/detail/seed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source   location  \\\n",
       "8119  internshala  hyderabad   \n",
       "8120  internshala     odisha   \n",
       "8121  internshala     odisha   \n",
       "8122  internshala     odisha   \n",
       "8123  internshala     odisha   \n",
       "\n",
       "                                               job_link  \n",
       "8119  https://internshala.com/internship/detail/digi...  \n",
       "8120  https://internshala.com/internship/detail/busi...  \n",
       "8121  https://internshala.com/internship/detail/cont...  \n",
       "8122  https://internshala.com/internship/detail/web-...  \n",
       "8123  https://internshala.com/internship/detail/seed...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of jobs collected\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source      0\n",
       "location    0\n",
       "job_link    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no null values in dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save it into normal csv file just run below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('first_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save file - pipe '|' as delimiter : just run the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('first_pipe_format.csv',sep ='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
